"""
XAUUSD Quantitative Analysis Script
====================================
Ingeniero de Software Cuantitativo - An√°lisis de Flujo de √ìrdenes y Proyecci√≥n de Volatilidad

Autor: Quant Engineer
Fecha: 10 de noviembre de 2025

M√≥dulos:
1. Identificaci√≥n Cuantitativa de S/R (Lot Profile & POIs)
2. Modelo de Volatilidad (GARCH)
3. Simulaci√≥n de Proyecci√≥n (Monte Carlo)
4. Integraci√≥n y An√°lisis Final
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
from arch import arch_model
import warnings
warnings.filterwarnings('ignore')


# ============================================================================
# M√ìDULO 1: IDENTIFICACI√ìN CUANTITATIVA DE S/R (LOT PROFILE & POIs)
# ============================================================================

def get_sync_pois(df_sync: pd.DataFrame, period_days: int) -> List[float]:
    """
    Extrae los niveles S/R "Absolutos" o Puntos de Inter√©s (POI) de alta prioridad
    basados en posiciones sincronizadas (>500 lotes).
    
    Args:
        df_sync: DataFrame con columnas ['timestamp', 'price', 'lots']
        period_days: N√∫mero de d√≠as hacia atr√°s para filtrar
        
    Returns:
        Lista de precios que representan POIs cr√≠ticos
    """
    try:
        # Asegurar que timestamp es datetime
        if not pd.api.types.is_datetime64_any_dtype(df_sync['timestamp']):
            df_sync['timestamp'] = pd.to_datetime(df_sync['timestamp'])
        
        # Calcular el cutoff date
        cutoff_date = df_sync['timestamp'].max() - pd.Timedelta(days=period_days)
        
        # Filtrar por per√≠odo
        df_filtered = df_sync[df_sync['timestamp'] >= cutoff_date].copy()
        
        if df_filtered.empty:
            print(f"‚ö† WARNING: No hay datos de sincronizaci√≥n en los √∫ltimos {period_days} d√≠as.")
            return []
        
        # Extraer los niveles de precio (ordenados por volumen de lotes)
        df_filtered = df_filtered.sort_values('lots', ascending=False)
        pois = df_filtered['price'].tolist()
        
        print(f"‚úì Extra√≠dos {len(pois)} POIs de eventos sincronizados (>{period_days}d)")
        return pois
        
    except Exception as e:
        print(f"‚úó ERROR en get_sync_pois: {str(e)}")
        return []


def build_lot_profile(df_micro: pd.DataFrame, 
                      period_days: int, 
                      bin_size_usd: float = 0.25) -> Dict[str, any]:
    """
    Crea un "Perfil de Lotes" (an√°logo a un Perfil de Volumen) para encontrar
    S/R basados en la distribuci√≥n de la liquidez.
    
    Args:
        df_micro: DataFrame con columnas ['timestamp', 'price', 'lots']
        period_days: N√∫mero de d√≠as hacia atr√°s para analizar
        bin_size_usd: Tama√±o del bin de precio en USD (default: $0.25)
        
    Returns:
        Diccionario con {'poc': float, 'hlns': [floats], 'llns': [floats]}
    """
    try:
        # Asegurar que timestamp es datetime
        if not pd.api.types.is_datetime64_any_dtype(df_micro['timestamp']):
            df_micro['timestamp'] = pd.to_datetime(df_micro['timestamp'])
        
        # Calcular el cutoff date
        cutoff_date = df_micro['timestamp'].max() - pd.Timedelta(days=period_days)
        
        # Filtrar por per√≠odo
        df_filtered = df_micro[df_micro['timestamp'] >= cutoff_date].copy()
        
        if df_filtered.empty:
            print(f"‚úó ERROR: No hay datos de microposiciones en los √∫ltimos {period_days} d√≠as.")
            return {'poc': None, 'hlns': [], 'llns': []}
        
        # Encontrar rango de precios
        min_price = df_filtered['price'].min()
        max_price = df_filtered['price'].max()
        
        # Crear bins de precio
        bins = np.arange(min_price, max_price + bin_size_usd, bin_size_usd)
        
        # Asignar cada precio a un bin
        df_filtered['price_bin'] = pd.cut(
            df_filtered['price'], 
            bins=bins, 
            labels=bins[:-1],
            include_lowest=True
        )
        
        # Agrupar por bin y sumar lotes
        lot_profile = df_filtered.groupby('price_bin')['lots'].sum().sort_index()
        
        # Convertir el √≠ndice a float
        lot_profile.index = lot_profile.index.astype(float)
        
        # Calcular estad√≠sticas
        mean_lots = lot_profile.mean()
        std_lots = lot_profile.std()
        
        # Identificar POC (Point of Control) - M√°ximo volumen
        poc_price = lot_profile.idxmax()
        
        # Identificar HLN (High Lotage Nodes) - > 1 std por encima de la media
        hln_threshold = mean_lots + std_lots
        hlns = lot_profile[lot_profile > hln_threshold].index.tolist()
        # Remover el POC de HLNs si est√° presente
        hlns = [h for h in hlns if h != poc_price]
        
        # Identificar LLN (Low Lotage Nodes) - < 1 std por debajo de la media
        lln_threshold = mean_lots - std_lots
        llns = lot_profile[lot_profile < lln_threshold].index.tolist()
        
        print(f"‚úì Perfil de Lotes construido ({period_days}d, bin=${bin_size_usd})")
        print(f"  - Rango de precios: ${min_price:.2f} - ${max_price:.2f}")
        print(f"  - Total de bins: {len(lot_profile)}")
        print(f"  - POC identificado: ${poc_price:.2f}")
        print(f"  - HLNs identificados: {len(hlns)}")
        print(f"  - LLNs identificados: {len(llns)}")
        
        return {
            'poc': float(poc_price),
            'hlns': [float(h) for h in hlns],
            'llns': [float(l) for l in llns],
            'profile': lot_profile  # Para an√°lisis adicional o visualizaci√≥n
        }
        
    except Exception as e:
        print(f"‚úó ERROR en build_lot_profile: {str(e)}")
        return {'poc': None, 'hlns': [], 'llns': []}


# ============================================================================
# M√ìDULO 2: MODELO DE VOLATILIDAD (GARCH)
# ============================================================================

def get_garch_forecast(df_ohlc: pd.DataFrame, 
                       timeframe_hours: float = 1.0) -> Optional[float]:
    """
    Calcula el r√©gimen de volatilidad actual y pronostica para el siguiente per√≠odo
    usando un modelo GARCH(1,1).
    
    Args:
        df_ohlc: DataFrame con columnas ['timestamp', 'open', 'high', 'low', 'close']
        timeframe_hours: Timeframe de las velas en horas (1.0 para H1, 0.25 para M15)
        
    Returns:
        Volatilidad diaria pronosticada (sigma) como porcentaje
    """
    try:
        # Verificar que hay suficientes datos
        min_observations = 200
        if len(df_ohlc) < min_observations:
            print(f"‚úó ERROR: Se necesitan al menos {min_observations} observaciones. Disponibles: {len(df_ohlc)}")
            return None
        
        # Calcular retornos logar√≠tmicos
        df_ohlc = df_ohlc.copy()
        df_ohlc['returns'] = np.log(df_ohlc['close'] / df_ohlc['close'].shift(1)) * 100
        
        # Eliminar NaNs
        returns = df_ohlc['returns'].dropna()
        
        if len(returns) < min_observations:
            print(f"‚úó ERROR: Retornos insuficientes despu√©s de limpieza: {len(returns)}")
            return None
        
        print(f"‚úì Calculando modelo GARCH con {len(returns)} observaciones...")
        
        # Ajustar modelo GARCH(1,1)
        # mean='Zero' asume que el retorno medio es cercano a cero (com√∫n en trading de alta frecuencia)
        model = arch_model(returns, vol='Garch', p=1, q=1, mean='Zero', rescale=False)
        
        # Entrenar el modelo
        res = model.fit(disp='off', show_warning=False)
        
        # Pronosticar volatilidad para el siguiente per√≠odo
        forecast = res.forecast(horizon=1)
        
        # Extraer la varianza pronosticada y convertir a desviaci√≥n est√°ndar
        variance_forecast = forecast.variance.values[-1, 0]
        volatility_period = np.sqrt(variance_forecast)
        
        # Escalar la volatilidad al timeframe diario
        # Si tenemos datos H1, hay 24 per√≠odos por d√≠a
        periods_per_day = 24.0 / timeframe_hours
        daily_volatility = volatility_period * np.sqrt(periods_per_day)
        
        print(f"‚úì Modelo GARCH ajustado exitosamente")
        print(f"  - Volatilidad del per√≠odo ({timeframe_hours}h): {volatility_period:.4f}%")
        print(f"  - Volatilidad diaria pronosticada: {daily_volatility:.4f}%")
        print(f"  - AIC: {res.aic:.2f}, BIC: {res.bic:.2f}")
        
        return daily_volatility
        
    except Exception as e:
        print(f"‚úó ERROR en get_garch_forecast: {str(e)}")
        return None


# ============================================================================
# M√ìDULO 3: SIMULACI√ìN DE PROYECCI√ìN (MONTE CARLO)
# ============================================================================

def run_monte_carlo(start_price: float,
                    daily_volatility: float,
                    drift: float = 0.0,
                    num_simulations: int = 10000,
                    num_steps: int = 24,
                    time_horizon: float = 1.0) -> np.ndarray:
    """
    Simula trayectorias de precios futuras usando Movimiento Browniano Geom√©trico (GBM).
    
    Args:
        start_price: Precio inicial de XAUUSD
        daily_volatility: Volatilidad diaria (sigma) del M√≥dulo 2, en porcentaje
        drift: Retorno medio esperado (mu), default 0.0 para neutralidad
        num_simulations: N√∫mero de trayectorias a simular
        num_steps: N√∫mero de pasos temporales
        time_horizon: Horizonte temporal en d√≠as
        
    Returns:
        Matriz NumPy (num_steps, num_simulations) con todas las trayectorias
    """
    try:
        print(f"‚úì Ejecutando simulaci√≥n Monte Carlo...")
        print(f"  - Precio inicial: ${start_price:.2f}")
        print(f"  - Volatilidad: {daily_volatility:.4f}%")
        print(f"  - Simulaciones: {num_simulations:,}")
        print(f"  - Pasos: {num_steps}")
        print(f"  - Horizonte: {time_horizon} d√≠as")
        
        # Calcular el incremento de tiempo
        dt = time_horizon / num_steps
        
        # Convertir volatilidad de porcentaje a decimal
        sigma = daily_volatility / 100.0
        mu = drift / 100.0
        
        # Generar matriz de n√∫meros aleatorios normales
        Z = np.random.standard_normal((num_steps, num_simulations))
        
        # Inicializar matriz de precios
        price_paths = np.zeros((num_steps + 1, num_simulations))
        price_paths[0] = start_price
        
        # Calcular trayectorias usando GBM
        # S_t = S_{t-1} * exp((mu - sigma^2/2)*dt + sigma*sqrt(dt)*Z_t)
        for t in range(1, num_steps + 1):
            drift_component = (mu - 0.5 * sigma**2) * dt
            diffusion_component = sigma * np.sqrt(dt) * Z[t-1]
            price_paths[t] = price_paths[t-1] * np.exp(drift_component + diffusion_component)
        
        # Eliminar el precio inicial (solo devolver proyecciones)
        price_paths = price_paths[1:]
        
        # Calcular estad√≠sticas de la simulaci√≥n
        final_prices = price_paths[-1]
        mean_final = np.mean(final_prices)
        median_final = np.median(final_prices)
        std_final = np.std(final_prices)
        
        print(f"‚úì Simulaci√≥n completada")
        print(f"  - Precio final promedio: ${mean_final:.2f}")
        print(f"  - Precio final mediana: ${median_final:.2f}")
        print(f"  - Desviaci√≥n est√°ndar final: ${std_final:.2f}")
        
        return price_paths
        
    except Exception as e:
        print(f"‚úó ERROR en run_monte_carlo: {str(e)}")
        return np.array([])


# ============================================================================
# M√ìDULO 4: INTEGRACI√ìN Y AN√ÅLISIS FINAL
# ============================================================================

def analyze_projection(mc_matrix: np.ndarray,
                       sr_levels_dict: Dict[str, any],
                       sync_pois_list: List[float],
                       start_price: float,
                       tolerance: float = 0.5) -> pd.DataFrame:
    """
    Calcula la probabilidad de que el precio interact√∫e con los S/R cuantitativos.
    
    Args:
        mc_matrix: Matriz de simulaciones Monte Carlo (num_steps, num_simulations)
        sr_levels_dict: Diccionario con POC, HLNs, LLNs del M√≥dulo 1.2
        sync_pois_list: Lista de POIs del M√≥dulo 1.1
        start_price: Precio inicial
        tolerance: Tolerancia en USD para considerar "toque" (default: $0.5)
        
    Returns:
        DataFrame con an√°lisis de probabilidades para cada nivel S/R
    """
    try:
        print(f"\n‚úì Analizando probabilidades de interacci√≥n con S/R...")
        
        if mc_matrix.size == 0:
            print("‚úó ERROR: Matriz de Monte Carlo vac√≠a")
            return pd.DataFrame()
        
        num_simulations = mc_matrix.shape[1]
        
        # Compilar todos los niveles S/R
        sr_levels = []
        
        # Agregar POC Micro (primario para trading)
        if sr_levels_dict.get('poc'):
            sr_levels.append({
                'price': sr_levels_dict['poc'],
                'type': 'POC_Micro',
                'priority': 'ALTA'
            })
        
        # Agregar POC Macro (contexto estructural)
        if sr_levels_dict.get('poc_macro'):
            sr_levels.append({
                'price': sr_levels_dict['poc_macro'],
                'type': 'POC_Macro',
                'priority': 'ALTA'
            })
        
        # Agregar HLNs Micro (accionables)
        for i, hln in enumerate(sr_levels_dict.get('hlns', []), 1):
            sr_levels.append({
                'price': hln,
                'type': 'HLN_Micro',
                'priority': 'MEDIA'
            })
        
        # Agregar HLNs Macro (contexto)
        for i, hln in enumerate(sr_levels_dict.get('hlns_macro', []), 1):
            sr_levels.append({
                'price': hln,
                'type': 'HLN_Macro',
                'priority': 'MEDIA'
            })
        
        # Agregar LLNs
        for i, lln in enumerate(sr_levels_dict.get('llns', []), 1):
            sr_levels.append({
                'price': lln,
                'type': 'LLN',
                'priority': 'BAJA'
            })
        
        # Agregar POIs sincronizados (Top 5 por importancia)
        for i, poi in enumerate(sync_pois_list[:5], 1):
            sr_levels.append({
                'price': poi,
                'type': 'POI',
                'priority': 'CR√çTICA'
            })
        
        if not sr_levels:
            print("‚ö† WARNING: No hay niveles S/R para analizar")
            return pd.DataFrame()
        
        # Calcular probabilidades para cada nivel
        results = []
        
        for level in sr_levels:
            price_level = level['price']
            
            # Calcular Probabilidad de Toque (Hit Probability)
            # ¬øCu√°ntas simulaciones cruzaron este nivel?
            hit_count = 0
            close_above_count = 0
            close_below_count = 0
            
            for sim_idx in range(num_simulations):
                path = mc_matrix[:, sim_idx]
                max_price = np.max(path)
                min_price = np.min(path)
                final_price = path[-1]
                
                # Determinar si hay toque (con tolerancia)
                if price_level > start_price:  # Nivel de resistencia
                    if max_price >= (price_level - tolerance):
                        hit_count += 1
                else:  # Nivel de soporte
                    if min_price <= (price_level + tolerance):
                        hit_count += 1
                
                # Determinar posici√≥n final
                if final_price > price_level:
                    close_above_count += 1
                else:
                    close_below_count += 1
            
            # Calcular probabilidades
            hit_probability = (hit_count / num_simulations) * 100
            close_above_prob = (close_above_count / num_simulations) * 100
            close_below_prob = (close_below_count / num_simulations) * 100
            
            # Determinar direcci√≥n del nivel respecto al precio actual
            direction = "RESISTENCIA" if price_level > start_price else "SOPORTE"
            distance = abs(price_level - start_price)
            distance_pct = (distance / start_price) * 100
            
            results.append({
                'Nivel': f"${price_level:.2f}",
                'Tipo': level['type'],
                'Prioridad': level['priority'],
                'Direcci√≥n': direction,
                'Distancia_USD': f"${distance:.2f}",
                'Distancia_%': f"{distance_pct:.3f}%",
                'Prob_Toque_%': f"{hit_probability:.2f}%",
                'Prob_Cierre_Arriba_%': f"{close_above_prob:.2f}%",
                'Prob_Cierre_Abajo_%': f"{close_below_prob:.2f}%",
                'Hit_Prob_Raw': hit_probability  # Para ordenar
            })
        
        # Crear DataFrame y ordenar por probabilidad de toque
        df_results = pd.DataFrame(results)
        df_results = df_results.sort_values('Hit_Prob_Raw', ascending=False)
        df_results = df_results.drop('Hit_Prob_Raw', axis=1)
        
        print(f"‚úì An√°lisis completado para {len(df_results)} niveles S/R")
        
        return df_results
        
    except Exception as e:
        print(f"‚úó ERROR en analyze_projection: {str(e)}")
        return pd.DataFrame()


# ============================================================================
# FUNCI√ìN PRINCIPAL: ORQUESTACI√ìN DEL AN√ÅLISIS COMPLETO
# ============================================================================

def run_complete_analysis(df_micro: pd.DataFrame,
                          df_sync: pd.DataFrame,
                          df_ohlc: pd.DataFrame,
                          lot_profile_days: int = 7,
                          sync_poi_days: int = 30,
                          bin_size: float = 0.25,
                          timeframe_hours: float = 1.0,
                          num_simulations: int = 10000,
                          num_steps: int = 24,
                          time_horizon: float = 1.0) -> Dict:
    """
    Ejecuta el an√°lisis cuantitativo completo de XAUUSD.
    
    Args:
        df_micro: DataFrame de microposiciones
        df_sync: DataFrame de posiciones sincronizadas
        df_ohlc: DataFrame de datos OHLC
        lot_profile_days: D√≠as para el perfil de lotes
        sync_poi_days: D√≠as para POIs sincronizados
        bin_size: Tama√±o del bin en USD
        timeframe_hours: Timeframe de las velas OHLC
        num_simulations: N√∫mero de simulaciones Monte Carlo
        num_steps: Pasos de la simulaci√≥n
        time_horizon: Horizonte temporal en d√≠as
        
    Returns:
        Diccionario con todos los resultados del an√°lisis
    """
    print("=" * 70)
    print("AN√ÅLISIS CUANTITATIVO XAUUSD - DEPTH EYE")
    print("=" * 70)
    print()
    
    # M√ìDULO 1: Identificaci√≥n de S/R
    print("[M√ìDULO 1] IDENTIFICACI√ìN CUANTITATIVA DE S/R")
    print("-" * 70)
    
    sync_pois = get_sync_pois(df_sync, sync_poi_days)
    lot_profile = build_lot_profile(df_micro, lot_profile_days, bin_size)
    
    print()
    
    # M√ìDULO 2: Modelo GARCH
    print("[M√ìDULO 2] MODELO DE VOLATILIDAD GARCH(1,1)")
    print("-" * 70)
    
    daily_volatility = get_garch_forecast(df_ohlc, timeframe_hours)
    
    if daily_volatility is None:
        print("‚úó AN√ÅLISIS ABORTADO: No se pudo calcular la volatilidad GARCH")
        return {}
    
    print()
    
    # M√ìDULO 3: Simulaci√≥n Monte Carlo
    print("[M√ìDULO 3] SIMULACI√ìN MONTE CARLO (GBM)")
    print("-" * 70)
    
    start_price = df_ohlc['close'].iloc[-1]
    mc_matrix = run_monte_carlo(
        start_price=start_price,
        daily_volatility=daily_volatility,
        drift=0.0,  # Neutral para trading de corto plazo
        num_simulations=num_simulations,
        num_steps=num_steps,
        time_horizon=time_horizon
    )
    
    if mc_matrix.size == 0:
        print("‚úó AN√ÅLISIS ABORTADO: Fallo en la simulaci√≥n Monte Carlo")
        return {}
    
    print()
    
    # M√ìDULO 4: Integraci√≥n y An√°lisis
    print("[M√ìDULO 4] INTEGRACI√ìN Y AN√ÅLISIS DE PROBABILIDADES")
    print("-" * 70)
    
    df_analysis = analyze_projection(
        mc_matrix=mc_matrix,
        sr_levels_dict=lot_profile,
        sync_pois_list=sync_pois,
        start_price=start_price
    )
    
    # Generar reporte final
    print()
    print("=" * 70)
    print("REPORTE FINAL - AN√ÅLISIS CUANTITATIVO XAUUSD")
    print("=" * 70)
    print()
    print(f"üìä CONTEXTO DEL MERCADO")
    print(f"   Precio Actual:                    ${start_price:.2f}")
    print(f"   Volatilidad Diaria (GARCH):       {daily_volatility:.4f}%")
    print(f"   Per√≠odo de An√°lisis (Lot Profile): {lot_profile_days} d√≠as")
    print(f"   Per√≠odo de An√°lisis (POIs Sync):   {sync_poi_days} d√≠as")
    print()
    
    print(f"üéØ NIVELES S/R IDENTIFICADOS")
    if lot_profile['poc']:
        print(f"   POC (Point of Control):           ${lot_profile['poc']:.2f}")
    print(f"   HLNs (High Lotage Nodes):         {len(lot_profile['hlns'])} niveles")
    print(f"   LLNs (Low Lotage Nodes):          {len(lot_profile['llns'])} niveles")
    print(f"   POIs Sincronizados (>500 lotes):  {len(sync_pois)} niveles")
    print()
    
    print(f"üé≤ PROYECCI√ìN MONTE CARLO")
    print(f"   N√∫mero de Simulaciones:           {num_simulations:,}")
    print(f"   Pasos Temporales:                 {num_steps}")
    print(f"   Horizonte:                        {time_horizon} d√≠a(s)")
    print()
    
    if not df_analysis.empty:
        print(f"üìà PROBABILIDADES DE INTERACCI√ìN CON S/R")
        print()
        print(df_analysis.to_string(index=False))
    else:
        print("‚ö† No se generaron resultados de probabilidad")
    
    print()
    print("=" * 70)
    
    # Retornar todos los componentes del an√°lisis
    return {
        'start_price': start_price,
        'daily_volatility': daily_volatility,
        'lot_profile': lot_profile,
        'sync_pois': sync_pois,
        'mc_matrix': mc_matrix,
        'probability_analysis': df_analysis
    }


# ============================================================================
# EJEMPLO DE USO Y DATOS DE PRUEBA
# ============================================================================

if __name__ == "__main__":
    """
    Secci√≥n de ejemplo con datos sint√©ticos para demostrar el flujo completo.
    En producci√≥n, reemplazar con datos reales de tu broker/feed.
    """
    
    print("Generando datos de prueba para demostraci√≥n...")
    print()
    
    # Generar datos sint√©ticos de microposiciones (>25 lotes)
    np.random.seed(42)
    n_micro = 5000
    
    dates_micro = pd.date_range(end=pd.Timestamp.now(), periods=n_micro, freq='5T')
    base_price_micro = 2350.0
    
    df_micro = pd.DataFrame({
        'timestamp': dates_micro,
        'price': base_price_micro + np.random.normal(0, 5, n_micro) + np.cumsum(np.random.normal(0, 0.1, n_micro)),
        'lots': np.random.exponential(50, n_micro) + 25
    })
    
    # Generar datos sint√©ticos de posiciones sincronizadas (>500 lotes)
    n_sync = 50
    dates_sync = pd.date_range(end=pd.Timestamp.now(), periods=n_sync, freq='12H')
    
    df_sync = pd.DataFrame({
        'timestamp': dates_sync,
        'price': base_price_micro + np.random.normal(0, 10, n_sync),
        'lots': np.random.exponential(200, n_sync) + 500
    })
    
    # Generar datos sint√©ticos OHLC (H1)
    n_ohlc = 2000
    dates_ohlc = pd.date_range(end=pd.Timestamp.now(), periods=n_ohlc, freq='1H')
    
    # Simular precios con proceso GARCH-like
    returns = np.random.normal(0, 1.2, n_ohlc)
    prices = base_price_micro * np.exp(np.cumsum(returns / 100))
    
    df_ohlc = pd.DataFrame({
        'timestamp': dates_ohlc,
        'open': prices,
        'high': prices + np.abs(np.random.normal(0, 2, n_ohlc)),
        'low': prices - np.abs(np.random.normal(0, 2, n_ohlc)),
        'close': prices + np.random.normal(0, 1, n_ohlc)
    })
    
    # Ejecutar an√°lisis completo
    results = run_complete_analysis(
        df_micro=df_micro,
        df_sync=df_sync,
        df_ohlc=df_ohlc,
        lot_profile_days=7,
        sync_poi_days=30,
        bin_size=0.50,  # $0.50 bins para XAUUSD
        timeframe_hours=1.0,  # H1 timeframe
        num_simulations=10000,
        num_steps=24,  # 24 horas
        time_horizon=1.0  # 1 d√≠a
    )
    
    # Los resultados est√°n disponibles en el diccionario 'results'
    # Puedes acceder a componentes individuales para an√°lisis adicional:
    # - results['start_price']
    # - results['daily_volatility']
    # - results['lot_profile']
    # - results['sync_pois']
    # - results['mc_matrix']
    # - results['probability_analysis']
